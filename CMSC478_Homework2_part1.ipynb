{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baldevoli/git_test/blob/main/CMSC478_Homework2_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a logistic regression model with the following hypothesis function that includes both linear and quadratic terms for the features:\n",
        "\n",
        "$$\n",
        "h_\\theta(x) = \\frac{1}{1 + e^{-(w_0 + w_1 x_1 + w_2 x_2^2 + w_3 x_3^3)}}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- \\(x_1\\) is the first feature\n",
        "- \\(x_2\\) is the second feature\n",
        "- \\(w_0, w_1, w_2, w_3\\) are the model parameters.\n"
      ],
      "metadata": {
        "id": "C4J5zEOGPO0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Log-Loss (Logistic Loss Function):\n",
        "The log-loss or binary cross-entropy loss function measures the performance of the classification model. This loss function penalizes wrong predictions and is based on the predicted probabilities output by the logistic regression model.\n",
        "\n",
        "$$\n",
        "\\mathcal loss(w) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- \\(m\\) is the number of training examples.\n",
        "- \\(y_i\\) is the true label for the \\(i\\)-th training example, where \\(y_i \\in \\{0, 1\\}\\).\n",
        "- \\(h_\\theta(x_i)\\) is the predicted probability for the \\(i\\)-th training example.\n"
      ],
      "metadata": {
        "id": "_rj7Pq5sQ5DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are given the following Python code that implements logistic regression without regularization."
      ],
      "metadata": {
        "id": "FJmAgMusQ9wA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part A: Run the code and print the loss"
      ],
      "metadata": {
        "id": "pFA2GTOLQGRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hypothesis function for logistic regression\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "n5CcOtNpUo-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eB3YZkUPMdI",
        "outputId": "845f3084-50d6-4d96-87ec-a8e584a7e219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 3.9479428218126125\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Logistic regression loss function\n",
        "def compute_loss(w, X, y):\n",
        "    m = len(y)\n",
        "    h = sigmoid(np.dot(X, w))\n",
        "    loss = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
        "    return loss\n",
        "\n",
        "# Example dataset\n",
        "X = np.array([[1, 2, 9], [2, 3, 16], [3, 4, 25]])  # Features\n",
        "y = np.array([0, 1, 0])  # Labels\n",
        "w = np.array([0.1, 0.2, 0.3])  # Initial weights\n",
        "\n",
        "# Calculate loss\n",
        "loss = compute_loss(w, X, y)\n",
        "print(\"Loss:\", loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute loss function to include L2 regularization.\n",
        "\n",
        "L2 Regularization (Ridge Regularization):\n",
        "In order to reduce overfitting, L2 regularization adds a penalty term to the loss function. The penalty is proportional to the sum of the squares of the model weights. This discourages large weight values and simplifies the model.\n",
        "\n",
        "Here, L2 regularization is represented as:\n",
        "\n",
        "$$\n",
        "\\text{Regularization Term} = \\frac{\\lambda}{2m} \\sum_{j=1}^{m} w_j^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- \\(\\lambda) is the regularization parameter, controlling the strength of the penalty.\n",
        "- \\( m \\) is the number of training examples.\n",
        "- \\( w_j \\) is the weight for the \\( j \\)-th feature.\n",
        "\n",
        "### Regularized Logistic Loss Function:\n",
        "\n",
        "When you combine the logistic loss with the L2 regularization term, you get the regularized loss function:\n",
        "\n",
        "$$\n",
        "\\mathcal{loss}_{\\text{reg}}(w) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{m} w_j^2\n",
        "$$\n",
        "\n",
        "In this equation:\n",
        "\n",
        "- The first part is the standard logistic loss function, which penalizes incorrect predictions.\n",
        "- The second part is the regularization term, which penalizes large values of the weights \\( w \\).\n"
      ],
      "metadata": {
        "id": "m5OSuTmxR6is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part B: Now Modify the compute_loss function to include L2 regularization(you can use different regularization parameters to see the difference)"
      ],
      "metadata": {
        "id": "EIDba68ES_pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss_with_l2(w, X, y, lambda_):\n",
        "    m = len(y)\n",
        "    h = sigmoid(np.dot(X, w))\n",
        "    loss = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
        "\n",
        "    #YOUR CODE HERE\n",
        "\n",
        "    regularization_term = (lambda_ / (2 * m)) * np.sum(w[1:] ** 2) # regularization term\n",
        "\n",
        "    loss += regularization_term  # added the regularization term on the loss function\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Regularization parameter\n",
        "lambda_ = 0.1  # You can adjust this value\n",
        "\n",
        "# Calculate regularized loss\n",
        "regularized_loss = compute_loss_with_l2(w, X, y, lambda_)\n",
        "print(\"Loss with L2 regularization:\", regularized_loss)\n"
      ],
      "metadata": {
        "id": "XG1sIKi4TXqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e275a39c-547c-4cae-f6ae-b9b046a9fa71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss with L2 regularization: 3.950109488479279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part C: Compute Gradient function with L2 regularization, explain how the regularization affects the weights ùë§"
      ],
      "metadata": {
        "id": "BbxTpJLGT-X8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent with L2 Regularization in Logistic Regression\n",
        "\n",
        "To calculate the updated weights in logistic regression with L2 regularization, follow these steps:\n",
        "\n",
        "#### 1. **Gradient of the logistic loss function without regularization:**\n",
        "\n",
        "The gradient of the logistic loss function with respect to the weights \\( w_j \\) (for \\( j = 0, 1, 2, \\dots \\)) is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_j} loss(w) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x_i) - y_i \\right) x_{ij}\n",
        "$$\n",
        "\n",
        "where \\( h_\\theta(x_i) \\) is the predicted probability for the \\( i \\)-th example, and \\( x_{ij} \\) is the \\( j \\)-th feature of the \\( i \\)-th example.\n",
        "\n",
        "#### 2. **L2 regularization term:**\n",
        "\n",
        "For L2 regularization, we add a penalty term proportional to the weights \\( w_j \\):\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_j} loss_{reg}(w) = \\frac{\\partial}{\\partial w_j} loss(w) + \\frac{\\lambda}{m} w_j\n",
        "$$\n",
        "\n",
        "#### 3. **Gradient Descent Update Rule:**\n",
        "\n",
        "To update the weights, we perform gradient descent, updating each weight \\( w_j \\) as follows:\n",
        "\n",
        "$$\n",
        "w_j := w_j - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x_i) - y_i \\right) x_{ij} + \\frac{\\lambda}{m} w_j \\right)\n",
        "$$\n",
        "\n",
        "This update rule ensures that each weight is penalized by its current value, effectively shrinking the weights during the update.\n"
      ],
      "metadata": {
        "id": "D9-Lk7S0YFCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient function with L2 regularization\n",
        "def compute_gradient_with_l2(w, X, y, lambda_):\n",
        "    m = len(y)\n",
        "    h = sigmoid(np.dot(X, w))\n",
        "\n",
        "    # Gradient of the loss function\n",
        "    gradient = (1/m) * np.dot(X.T, (h - y))\n",
        "\n",
        "    #YOUR CODE HERE\n",
        "    gradient[1:] += (lambda_ / m) * w[1:]  # added the penalty term  to the gradient descent.\n",
        "\n",
        "    return gradient\n"
      ],
      "metadata": {
        "id": "oWLdI7xYYqKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent Update with L2 regularization\n",
        "def gradient_descent(X, y, w, alpha, lambda_, num_iters):\n",
        "    for i in range(num_iters):\n",
        "        # Compute the gradient with L2 regularization\n",
        "        grad = compute_gradient_with_l2(w, X, y, lambda_)\n",
        "        # Update the weights\n",
        "        w -= alpha * grad\n",
        "    return w"
      ],
      "metadata": {
        "id": "-A4YYZrDZI95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.01  # Learning rate\n",
        "lambda_ = 0.1  # Regularization parameter # You can adjust this value\n",
        "num_iters = 1000  # Number of iterations. # You can adjust this value\n",
        "\n",
        "# Perform gradient descent to update weights\n",
        "updated_w = gradient_descent(X, y, w, alpha, lambda_, num_iters)\n",
        "print(\"Updated weights after regularization:\", updated_w)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BizlSA2xZf0T",
        "outputId": "e5c3bf11-437a-46dc-efd1-5863d409325b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated weights after regularization: [ 0.33101155  0.2095138  -0.11602928]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain explain how the regularization affects the weights ùë§\n",
        "\n",
        "L2 Regularization helps to prevent overfitting by adding a penalty to large weight value . We also modified the loss function to use regularization which makes the weight small. Gradient descent  function have\n",
        "$$\n",
        " \\frac{\\lambda}{m} w_j  \n",
        "$$\n",
        "which penalty the large weight force it to shrink.Which increase the bias and decrease the variance which helps to prevent the overfitting.So it helps to make a balance between the bias and variance.So regularization can help to prevent overfitting by controlling model complexity and improving its generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "phTj2ozADkW-"
      }
    }
  ]
}